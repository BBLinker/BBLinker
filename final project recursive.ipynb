{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact_manual\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import concatenate, add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\envs\\tensorflow-gpu\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords=[]\n",
    "with open('stopwords.txt', 'r', encoding='utf8') as file:\n",
    "    for data in file.readlines():\n",
    "        data = data.strip()\n",
    "        stopWords.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Word2Vec.load(\"word2vec100維度.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=load_model('my_modelLSTM20190506.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "V=model.wv.vocab\n",
    "VV=list(V.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "種類: 生活 標題: 要求伊朗協商放棄核武 川普：不排除軍事對抗\n",
      "神經網路判斷最可能為: 國際 神經網路判斷第二可能為: 政治\n"
     ]
    }
   ],
   "source": [
    "y='要求伊朗協商放棄核武 川普：不排除軍事對抗'\n",
    "x='生活'\n",
    "def f(x,y):\n",
    "    W=y\n",
    "    r1 = u'[a-zA-Z0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…【】〈〉～「」é；《》？“”‘’！[\\\\]^_`{|}~]+'\n",
    "    r2 = \"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\"\n",
    "    r3 =  \"[.!//_,$&%^*()<>+\\\"'?@#-|:~{}]+|[——！\\\\\\\\，。=？、：“”‘’《》【】￥……（）]+\"\n",
    "    r4 =  \"\\\\【.*?】+|\\\\《.*?》+|\\\\#.*?#+|[.!/_,$&%^*()<>+\"\"'?@|:~{}#]+|[——！\\\\\\，。=？、：“”‘’￥……（）《》【】]\"\n",
    "    \n",
    "    y=re.sub(r1,'',y)\n",
    "    y=re.sub(r2,'',y)\n",
    "    y=re.sub(r3,'',y)\n",
    "    y=re.sub(r4,'',y)\n",
    "    \n",
    "    jieba.cut(y)\n",
    "    F=[]\n",
    "    for j in jieba.cut(y):\n",
    "        F.append(j)\n",
    "    \n",
    "    remainderWords =list(filter(lambda a: a not in stopWords and a != '\\n', F))\n",
    "    G=remainderWords\n",
    "    \n",
    "    H=[]\n",
    "    if len(G)>=10:\n",
    "        H=G[:10]\n",
    "    else:\n",
    "        H=G\n",
    "    \n",
    "    TT=[]\n",
    "\n",
    "    for j in H:\n",
    "        if j in VV:\n",
    "            TT.append(model.wv.__getitem__(j))\n",
    "        else:\n",
    "            n=np.zeros(100)\n",
    "            TT.append(n)\n",
    "            \n",
    "    T2=[]\n",
    "    if len(TT)==10:\n",
    "        T2=TT\n",
    "    else:\n",
    "        c=10-len(TT)\n",
    "        for j in range(1,c+1):\n",
    "            TT.append(np.zeros(100))\n",
    "        T2=TT\n",
    "    \n",
    "    T3=[]\n",
    "    z=list(T2)\n",
    "    for j in z:\n",
    "        T3.append(list(j))\n",
    "    D=[]\n",
    "    D=np.array(T3)\n",
    "    \n",
    "    E=D.reshape(1,10,100)\n",
    "\n",
    "    MP=model1.predict(E)[0]\n",
    "    RMP=list(MP).index(max(list(MP)))\n",
    "    S=MP\n",
    "    S[RMP]=0\n",
    "    RMP2=list(MP).index(max(list(S)))\n",
    "    \n",
    "    M=['娛樂','生活','社會','政治','財經','國際','體育']\n",
    "    print('種類:',x,'標題:',W)\n",
    "    print('神經網路判斷最可能為:',M[RMP],'神經網路判斷第二可能為:',M[RMP2])\n",
    "    \n",
    "f(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 去逗點"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = u'[a-zA-Z0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…【】〈〉～「」é；《》？“”‘’！[\\\\]^_`{|}~]+'\n",
    "r2 = \"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\"\n",
    "r3 =  \"[.!//_,$&%^*()<>+\\\"'?@#-|:~{}]+|[——！\\\\\\\\，。=？、：“”‘’《》【】￥……（）]+\"\n",
    "r4 =  \"\\\\【.*?】+|\\\\《.*?》+|\\\\#.*?#+|[.!/_,$&%^*()<>+\"\"'?@|:~{}#]+|[——！\\\\\\，。=？、：“”‘’￥……（）《》【】]\"\n",
    "\n",
    "y=re.sub(r1,'',y)\n",
    "y=re.sub(r2,'',y)\n",
    "y=re.sub(r3,'',y)\n",
    "y=re.sub(r4,'',y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.cut(y)\n",
    " F=[]\n",
    "    for j in jieba.cut(y):\n",
    "        F.append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 去除停用詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=[]\n",
    "for i in range(len(F)):\n",
    "    remainderWords = list(filter(lambda a: a not in stopWords and a != '\\n', F[i]))\n",
    "    G.append(remainderWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 調整長度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H=[]\n",
    "if len(G)>=10:\n",
    "    H.append(G[:10])\n",
    "else:\n",
    "    H.append(G)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 填充向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TT=[]\n",
    "\n",
    "for j in H:\n",
    "    if j in VV:\n",
    "        TT.append(model.wv.__getitem__(j))\n",
    "    else:\n",
    "        n=np.zeros(100)\n",
    "        TT.append(n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 補滿10*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2=[]\n",
    "if len(TT)==10:\n",
    "    T2.append(TT)\n",
    "else:\n",
    "    c=10-len(TT)\n",
    "    for j in range(1,c+1):\n",
    "        TT.append(np.zeros(100))\n",
    "    T2.append(TT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 轉乘np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T3=[]\n",
    "z=list(T2)\n",
    "for j in z:\n",
    "    T3.append(list(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=[]\n",
    "D=np.array(T3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.predict(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.predict(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([5,3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.array([5,3,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=re.sub(r1,'',y)\n",
    "y=re.sub(r2,'',y)\n",
    "y=re.sub(r3,'',y)\n",
    "y=re.sub(r4,'',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1=[]\n",
    "for i in A:\n",
    "    i=re.sub(r1,'',i)\n",
    "    A1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2=[]\n",
    "for i in A1:\n",
    "    i=re.sub(r2,'',i)\n",
    "    A2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A3=[]\n",
    "for i in A2:\n",
    "    i=re.sub(r3,'',i)\n",
    "    A3.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A4=[]\n",
    "for i in A3:\n",
    "    i=re.sub(r4,'',i)\n",
    "    A4.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A5=[]\n",
    "for i in A4:\n",
    "    A5.append(jieba.cut(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=[]\n",
    "a=0\n",
    "for i in range(len(A5)):\n",
    "    F=[]\n",
    "    for j in A5[i]:\n",
    "        F.append(j)\n",
    "    C.append(F)\n",
    "    a=a+1\n",
    "    print(a,end='\\r'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1=[]\n",
    "a=0\n",
    "for i in range(len(C)):\n",
    "    remainderWords = list(filter(lambda a: a not in stopWords and a != '\\n', C[i]))\n",
    "    C1.append(remainderWords)\n",
    "    a=a+1\n",
    "    print(a,end='\\r'),"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-gpu4)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
